---------------------------------
--- Module 1: Neural Networks ---
---------------------------------

--------------------------------------------------------
  Neural Networks Part 1: Setting up the Architecture
--------------------------------------------------------

Quick Intro:

- s = W2 * max(0, W1 * x)

- the function max(0, -) is a non-linearity that is applied elementwise. There are 
several choices for the non-linearity, but max is a common choice and simply thresholds
all activations that are below zero to zero.

Notice that the non-linearity is critical computationally, if left out, the two matrices
could be collapsed to a single matrix and the predicted class scores would again be a 
linear function of the input.

The non-linearity is where we get the wiggle.

- the parameters W2, W2 are learned with SGD, and their gradients are derived with 
chain rule(and computed with backpropagation).

- A 3-layer NN could look like s = W3 * max(0, W2 * max(0, W1 * x)), where all of W3,
W2, W1 are parameters to be learned

- sizes of the intermediate hidden vectors are hyperparameters of the network, which 
will be discussed later.


Modeling one neuron:

- area of Neural Nets was primarily inspired for modeling biological neural systems but
has since diverged and become a matter of engineering and achieving good results in ML tasks

- basic computational unit of the brain is a !neuron! (86 billion approx. in the nervous sys.)

- connected with approx. 10^14 - 10^15 !synapses!

- each neuron received input signals from its !dendrites!

- produces output signals along its single !axon!

- axon branches out and connects via synapses to dendrites of other neurons

- the signals that travel along the axons (x0) interact multiplicatively (e.g. w0x0) 
with dendirites of other neuron based on the synaptic strength at the synapse (w0)

- idea is that synaptic strengths are learnable and control strength of influence

- dendrites carry signal to the cell body where they all get summed

- if final sum is above certain threshold, the neuron can fire, sending a spike along its axon

- we model the firing rate of the neuron with an !activation function f!

- Coarse model - this model of a biological neuron is very coarse: 

a. many different types of neurons, with different properties

b. dendrites in biological neurons perform complex nonlinear computations

c. synapses are not just a single weight, they're complex non-linear dynamical systems

d. exact timing of the output spikes in many systems is known to be important, suggesting
that the rate code approximation may not hold


Single neuron as a linear classifier:

- A single neuron can be used to implement a binary classifier

- e.g. binary Softmax or binary SVM classifiers


Commonly used activation functions/non-linearity:

a. Sigmoid 

- squashes real-valued number into range between 0 and 1

- large neg nums become 0 and large pos number become 1

- has seen frequent use historically since it has a nice interpretation as the firing rate
of a neuron (0 - 1), but has fallen out of favor and is rearely ever used

- two major drawbacks: 


(-) Sigmoid saturate and end up killing gradients

- when neuron's activation saturates at either end of 0 or 1, the gradient at these regions
is almost zero (flat slope)

- recall that during backprop, this local gradient will be multiplied to the gradient of
this gate's output for the whole objective

- therefore if local gradient is very small, it will effectively "kill" the gradient and
almost no signal will flow through the neuron to its weights and recursivle to its data

- one must pay extra caution when initializing the weights of sigmoid neurons to prevent
saturation,

if initial weights are too large then most neurons would become saturated and the network
will barely learned



(-) Outputs are not zero-centered 

- neurons in later layers of processing would be receiving data that is not zero-centered

- this is a problem during gradient descent, because if data coming into a neuron is always
positive, then the gradient on the weights w will either be all positive, or all negative
during backprop

- this could introduce undesirable zig-zagging dynamics in the gradient updates for the 
weights

- however, once these gradients are added up across a batch of data, the final update for the
weights can have variable signs, somewhat mitigating this issue

- thus, this problem has less severe consequences compared to the saturated activation
above


b. Tanh

- squashes a real-valued number to the range [-1, 1] 

- therefore in practice, the tanh non-linearity is always preferred to the sigmoid 
non-linearity

- Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the 
following holds, tanh(x) = 2o(2x) - 1

- pros and cons:

(-) Like the sigmoid neuron, its activations saturate


(+) But unlike the sigmoid neuron, its output is zero-centered


c. ReLU 

- the Rectified Linear Unit has become very popular in the last few years

- computes the function f(x) = max(0,x)

- in other words, the activation is simple thresholded at zero

- pros and cons:

(+) Found to greatly accerlerate the convergence of SGD compared to sigmoid and tanh
    functions. Argued that it is due to its linear, non-saturating form 

(+) Compared to tanh/sigmoid neurons that involve expenseive operations, the ReLU can 
    be implemented by simple thresholding a matrix of activations at zero

(-) Unfortunately, ReLU units can be fragile during training and can "die".

    - for example, a large gradient flowing through a ReLU neuron could cause the weights 
      to update in such a way that the neuron will never activate on any datapoint again

    - if this happens, then the gradient flowing through the unit will forever be zero 
      from that point on. That is, the ReLU units can irreversible due during training 
      since they can get knocked off the data manifold.

    - for example, you may find that as much as 40% of your network can be "dead"
      (i.e. neurons that never activate across the entire training dataset) if the learning
      rate is set too high

    - with a proper setting of the learning rate this is less frequently an issue

d. Leaky ReLU

    - Leaky ReLUs are one attempt to fix the "dying ReLU" problem

    - Instead of the function being zero when x < 0, a leaky ReLU will instead have a small
      negative slope (of 0.01, or so)

    - That is, the function computse f(x) = 1(x < 0)(ax) + 1(x >= 0)(x) where a is a small
      constant

    - some people report success with this form of activation function,but the results are
      not always consistent

    -  the slope in the negative region can also be made into a parameter of each neuron,
       as seen in PReLU neurons

e. Maxout 

	- other types of units have been proposed that do not have the functional form 
	  where a non-linearity is applied on the dot product between the weights and the data

	- one relatively popular choice is the Maxout neuron (introduced recently by 
	  Goodfellow et. al.) that generalizes the ReLU and its leaky version.

	- the Maxout neuron computes the function max(w1x + b1, w2x + b2)

	- notice that both ReLU and Leaky ReLU are a special case of this form, (for example,
      for ReLU we have w1,b1 = 0)

	- the Maxout neuron enjoys all benefits of a ReLU unit (linear regime of operation,
	  no saturation) and does not have its drawbacks (dying ReLU)

	- However, unlike the ReLU neurons it doubles the number of parameters for every single
	  neuron, leading to a high total number of parameters

<TLDR:> - use the ReLU non-linearity
		- be careful with your learning rates if you do so 
		- possibly monitor the fraction of "dead" units in the network 
		- of this concerns oyu, give Leaky ReLU or Maxout a try
		- never use sigmoid, try tanh but expect it to work worse than ReLU/Maxout


Neural Network Architectures:

a. Layer Wise Organization

- most common layer type is the #fully-connected# layer in which neurons between two
  adjacent layers are fully pairwise connected

- #Naming conventions# input layer is not counted

- therefore, a single-layer neural network describes a network with no hidden layers
  (input directly mapped to output)

- in that sense, you can sometimes hear people say the logistic regression or SVMs are 
  simply a special case of single-layer neural networks 

- unlike all layers in a neural network, the output layer neurons most commonnly do not
  have an activation function

- this is because the last layer is usualy reserved to to represent the class scores 
   (e.g. classification)

- the two metrics that people commonly use to measure the size of neural networks are the:

	1. number of neurons
	2. more commonly, the number of params 

	* first network (left) has 4 + 2 = 6 neurons(not counting the inputs), [3x4] + [4x2]
	  = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters

- to give some context, modern Convolutional Networks contain on orders of 100 million 
  parameters and are usuallu made up of approximately 10-20 layers (hence deep learning)

b. Example feed-forward computations

	[py]
		# forward-pass of a 3-layer neural network:
		f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
		x = np.random.randn(3, 1) # random input vector of three numbers (3x1)
		h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)
		h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)
		out = np.dot(W3, h2) + b3 # output neuron (1x1)
	[end]

	- the forward pass of a fully-connected layer corresponds to one matrix multiplication
	  followed by a bias offset and activation function


c. Representational Power

	- one way to look at neural networks with FC layers is that they define a family of
	  functions that are parameterized by the weights of the network 

	- a natural question that arises is: 

	  <What is the representational power of this family of functions?>

	  <In particular, are there functions that cannot be modeled with a Neural Network?>

	- it turns out that neural nets with at least one hidden layer are universal approx.

	- 

d. Setting number of layers and their sizes

	- note that as we increase the size and number of layers in a Neural Network, the
	  # capacity # of the network increases

	- that is, the space of representable functions grows since the neurons can collaborate
	  to express many different functions 

	- neural networks with more neurons can express more complicated functions

	- this is both a blessing and a curse however

	- for example, the model with 20 hidden neurons fits all the training data but at the 
	  cost of segmenting the space into many disjoint red and green decision regions 

	- the model with 3 hidden neurons only has the representational power to classify the
	  data in broad strokes

	- it models the data as two blobs and inteprets the few red points inside the green
	  cluster as # outliers (noise) #

	- in practice, this could lead to better # generalization # on the test set 

	- based on our discussion above, it seems that smaller neural networks can be 
	  preferred if the data is not complex enough to prevent overfitting

	- however, this is incorrect, there are many other preferred ways to prevent overfitting
      in neural networks that we will discuss later (L2 reg, dropout, input noise)

	- in practice, it is always better to use these methods to control overfitting instead
	  of the number of neurons

	- the subtle reason behind this is that smaller networks are harder to train with local
	  methods such as Gradient descent

	- it's clear that their loss functions have relatively few local minima but it turns
	  out that many of these minima are easier to converge to, but they are bad
	  (i.e. high loss)

	- conversely, bigger neural nets contains significantly more local minima, but these
	  minima turn out to be much better in terms of their actual loss

	- since neural nets are non-convex, it's hard to study these properties mathematically

	- but some attempts to understand these objective functions have been made, e.g. in 
	  a recent paper "The Loss Surfaces of Multilayer Networks"

	- in practice, what you find is that if you train a small network the final loss can 
	  display a good amount of variance

	- in some cases you get lucky and converge to a good place but in some cases you get 
	  trapped in one of the bad minima

	- conversely, if you train a large network you'll start to find many different solutions
	  but the variance in the final achieved loss will be much smaller. In other words, all
	  solutions are about equally as good and rely less on the luck of random initialization

	- to reiterate, the regularization strength is the preferred way to control the 
	  overfitting of a neural network

	<TLDR>: The takeaway is that you should not be using smaller networks because you are 
			afraid of overfitting, instead you should use as big of a neural network as your
			computational budget allows, and use other regularization techniques to control
			overfitting			


Summary:

1. introduced a very coarse model of a biological neuron 
2. several types of activation funccs that are used in practice (ReLU) being the most
   common choice
3. introduced neural nets where neurons are connected with Fully-Connected Layers
4. we saw that this layered architecture enables very efficient evaluation of Neural Nets 
   based on matrix multiplications interwoven with the application of the activation
   function 
5. Neural Nets are # universal function approximators # but we also discussed the fact that 
   this property has little to do with their ubiquitous use. They are used because they make
   certain "right" assumptions about the functional forms of functions that come up in 
   practice
6. Larger networks will always work better than smaller networks, but their higher model 
   capacity must be appropriately addressed with stronger regularization(such as higher
   weight decay), or they might overfit.    

   We will see more forms of regularization (especially dropout) in later sections.



-----------------------------------------------------------
  Neural Networks Part 2: Setting up the Data and the Loss
-----------------------------------------------------------

Setting up the data and the model:


a. Data Preprocessing

- 3 common forms of data preprocessing a data matrix X (assume size [N x D]):

  1. Mean subtraction

	- most common form of preprocessing 

	- involves subtracting the mean across every individual feature in the data 

	- has the geometric interpretation of centering the cloud of data around the origin 
	  along every dimension

	[py] 
		X -= np.mean(X, axis = 0) 
	[end]

  2. Normalization 

	- refers to normalizing the data dimensions so that hey are of approximately the same 
  	  scale

	- two common ways of achieving this normalization

		I. Divide each dimension by its standard deviation, once it has been zero-centered:

		   [py] 
		   		X /= np.std(X, axis=0) 
		   [end]

		II. Another form of this preprocesssing normalizes each dimension so that the min
			and max along each dimension is -1 and 1 respectively

	- it only makes sense to apply this preprocessing if you have a reason to 
			  believe that different input features have different scales

	- in the case of images, the relative scales of pixels are already approx. equal
	  (and in range from 0 to 255)


  3. PCA and Whitening 

	- another form of preprocessing 

	- in this process, the data is first centered as described above

	- then the covariance matrix is computed that tells us about the correlation structure
  	  in the data

  	 [py]
  	  # Assume input data matrix X of size [N x D]
	  X -= np.mean(X, axis = 0) # zero-center the data (important)
	  cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix
  	 [end]

  	- the (i,j) element of the data covariance matrix maintains the covariance between 
  	 i-th and j-th dimension of the data 

    - in particular, the diagonal of this matrix contains the variances. 

    - Furthermore, the covariance matrix is symmetric and positive semi-definite

    - etc. etc. didn't complete the notes since PCA/Whitening are not used with ConvNets 

  <Common pitfall>: any preprocessing statistics (e.g. the data mean) must only be computed
  					on the training data, and then applied to the validation / test data 
  					
  					e.g. computing the mean and subtracting it from every image across the 
  					entire datset and then splitting the data into tran/val/test splits 
  					would be a mistake. Instead, the mean must be computed only over the 
  					training data and then subtracted equally from all the splits 
  					(train/val/test)


b. Weight Initialization 

	I. Pitfall: all zero initialization. With proper data normalization, it is reasonable
	   to assume that approximately half of the weights will be psoitivve and half of them
	   will be negative.

		- zero might have the expectation to be the "best guss" but this is a mistake, 
		  because if every neuron in the network computes the same output, then they will 
		  also all compute the same gradients during backprop and udnergo the exact same 
		  param updates

		- in other words, there is no source of asymmetry between neurons if their weights 
		  are initialized to be the same

    II. Small random numbers

		- thus, we still want the weights to be very close to zero, but as we have argued 
		  above, not identically zero.

		- As a solution, it is common to initialize the weights of the neurons to small 
		  numbers and refer to doing so as symmetry breaking

		- the idea is that neurons are all random and unique in the beginning so they will 
		  compute distinct updates and integrate themselves as diverse parts of the full
		  network

		- [py] W = 0.01* np.random.randn(D,H) 
		  [end]

		- where randn samples from a zero mean, unit standard dev. gaussian. With this
		  formulation, every neuron's weight vector is initialized as a random vector 
		  sampled from a multi-dimensional gaussian, so the neurons point in random 
		  directions in the input space

		- it is also possible to use small numbers drawn from a uniform distribution,
		  but this seems to have relatively little impact on the final performance in
		  practice

		- #WARNING#: not necessarily that smaller nums will work strictly better. For 
		  example, a neural netwok that has very small weight will compute very small 
		  gradients on its data. this could greatly diminish the "gradient signal" 
		  flowing backward through a network

	III. Calibrating the variances with 1/sqrt(n)

		- one problem is that the distribution of the outputs from a randomly
		initialized neuron has a variance that grows with number of inputs

		- we can normalize variance of each neuron's output to 1 by scaling its weight
		vector by the square root of its fan-in(number of inputs)

		- ensures all neurons in network initially have approximately the same 
		output distrib and empirically improves rate of convergence

		[py] 

			w = np.random.randn(n) / sqrt(n)

		[end]

	
	IV. Sparse Initialization

		- another way to solve uncalibrate variances problem

		- set all W matrices to zero

		- but to break symmetry every neuron must be randomly connected 
		to a fixed num of neurons below it. 

		- typical num of neurons to connect to may be as small as 10

	V. Initializing the biases

		- possible and common to initialize biases to zero

		- since asymmetry breaking is provided by small random nums in the weights

		- for ReLU non-linearities, some like to use a small value like 0.01 for all
		biases cause this ensures all ReLU units fire at the start and this, 
		obtain and propagate some gradient

		- but no clear eveidence that this provides consistent improvement

		- more common to simple use 0 bias initialization 

    VI. In practice

    	- curr recommendation is use ReLU units and use 
    	  [py]

    	  	w = np.random.randn(n) * sqrt(2.0/n)

    	  [end]

  		- this is as discuessed in He et al.

  	VII. Batch Normalization 

  		- alleviates headeaches with properly initializing neural nets 

  		- explicitly forces activations in the network to take on a unit gaussian 
  		distribustion at the start of training

  		- idea is this is possible cause normalization is a simple differentiable 
  		operation 

  		- usually insert BatchNorm layer immediately after fully connected layers 
  		(or convolutional layers) and before non-linearities

  		- networks that use this are significantly more robust to bad initialization 

  		- can be interpreted as doing preprocessing at every layer of the network
  		but integrated into the network itself in a differentiable manner

c. Regularization 

	I. L2 regularization

		- most common form of reg 

		- can be implemented by penalizing the squared magnitude of all params 

		- that is, for every weight w in the network, we add the 
		(1/2) * lambda * w^2 to the objective where lambda is reg strength

		- common to see 1/2 in front to cancel the 2 that results from the 
		differentiation

		- intuitive interpretation of heavily penalizing peaky weight vectors 

		- preferring diffuse weight vectors 

		- desirable property of encouraging network to use all of its inputs a little
		rather than some of its inputs a lot 

		- notice during grad desc every weight is decayed linearly W += -lambda * W 

	II. L1 regularization 

		- add the term lambda | w | to the objective 

		- possible to combine L1 and L2 regularization called # Elastic net reg #

		- can lead the weight vectors to become sparse during optimization 
		(unlike L2 which discourages sparsity). L1 DOES NOT encourage sparsity, 
		it's more that L2 discourages it

		- if not conerned with explicit feature selection, L2 reg can be expected 
		to give superior performance over L1 

	III. Max norm constraints

		- enforces an absolute upper bound on the magnitude of weight vector for 
		every neuron

		- uses projected gradient descent to enforce the constraint 

		- this corresponds to performing parameter update as normal, then enforcing
		the constraint by clamping the weight vector of every neuron to satisfy 
		||w||2 < c 

		- typical vals of c are on orders of 3 or 4 

		- some people report improvements 

		- appealing property where network cannot explode even when learning rates 
		are set too high because updates are always bounded

	IV. Dropout 

		- no one uses this anymore, skipped 

	V. Bias regularization

		- not common to regularize bias because they do not interact with data through 
		multiplicative interactions

		- does not have the interpretation of controlling the influence of a data dimension
		on the final objective

		- in practical applications, regularizing the bias rarely leads to significantly
		worse performance 

		- this is likely because there are very few bias terms compared to all the weights
		so the classifier can "afford to" use the biases if it needs them to obtain a
		better data loss

	VI. Per-layer regularization

		- not very common to regularize different layers to different amounts

		- relatively few results regarding this idea have been published in the 
		literature 

	<In practice:> most common to use a single, global L2 regularization strength 
	that is cross-validated. The value of p = 0.5 is a reasonable default, but this 
	can be tuned on validation data 


d. Loss Functions 

	- regularization loss has been discussed, second part is the data loss which
	is a supervised learning problem that measures compatibility between a pred 
	and the ground truth label 

	- data loss takes the form of an average over the data losses for every 
	individual example. That is, L = (1/N) summation of (Li)

	- where N is the number of training data 

	I. Classification

		- dataset of examples and a single correct label out of a fixed set for 
		each example 

		- commonly seen cost functions in this setting is the SVM (hinge loss)

		- also, some people report better perform with the squared hinge loss 

		- another common choice is the softmax classifier that uses cross-entropy
		loss

		# Problem: large number of classes #

			- when set of labels is very large (e.g. words in dictionary, ImageNet
			which has 22,000 categories) it may be helpful to use *Hierarchical Softmax*

			- this decomposes labels into a tree 

			- each label is represented as a path along the tree 

			- a Softmax classifier is trained at every node of the tree to disambiguate
			between left and right branches

			- structure of tree strongly impacts performance and is generally problem-
			dependent

		# Attribute classification

			- both losses above assume there is a single correct answer yi

			- what if yi is a binary vector where every example may or may not 
			have a certain attribute, and where the attributes are not exclusive?

			- a sensible approach in this case is to build a binary classifier for 
			every single attribute independently 

			- an alternative to this loss would be to train a logistic regression 
			classifier for every attribute independently 

	II. Regression 

		- task of predicting real-valued quantities such as the price of houses

		- common to compute the loss between predicted quantity and true answer then,
		measure the L2 squared norm, or L1 norm of the difference

		# Word of caution: # 

			- important to note that the L2 loss is much harder to optimize than
			a more stable loss such as Softmax

			- requires a very fragile and specific property from the network to  
			output exactly one correct value for each input

			- not the case with Softmax, where the precise value of each score is less
			important

			- only matters that their magnitudes are appropriate

			- L2 loss is less robust because outliers can introduce huge gradients

			- when faced with a regression problem, *consider if absolutely necessary*

			- instead, have a strong preference to discretizing outputs to bins and 
			perform classification over them whenver possible

			e.g. predicting star rating, it might be better to use 5 independent 
			classifiers for ratings of 1-5 stars instead of regression 

			- classification has the additional benefit that it can give you a distrib 
			over the regression outputs, not just a single output with no indication 
			of its confidence

			- if certain that classification is not appropriate, use L2 but be careful.

			- for example, L2 is more fragile and applying dropout in the network 
			is not a great idea



