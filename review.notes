---------------------------------
--- Module 1: Neural Networks ---
---------------------------------

--------------------------------------------------------
  Neural Networks Part 1: Setting up the Architecture
--------------------------------------------------------

Quick Intro:

- s = W2 * max(0, W1 * x)

- the function max(0, -) is a non-linearity that is applied elementwise. There are 
several choices for the non-linearity, but max is a common choice and simply thresholds
all activations that are below zero to zero.

Notice that the non-linearity is critical computationally, if left out, the two matrices
could be collapsed to a single matrix and the predicted class scores would again be a 
linear function of the input.

The non-linearity is where we get the wiggle.

- the parameters W2, W2 are learned with SGD, and their gradients are derived with 
chain rule(and computed with backpropagation).

- A 3-layer NN could look like s = W3 * max(0, W2 * max(0, W1 * x)), where all of W3,
W2, W1 are parameters to be learned

- sizes of the intermediate hidden vectors are hyperparameters of the network, which 
will be discussed later.


Modeling one neuron:

- area of Neural Nets was primarily inspired for modeling biological neural systems but
has since diverged and become a matter of engineering and achieving good results in ML tasks

- basic computational unit of the brain is a !neuron! (86 billion approx. in the nervous sys.)

- connected with approx. 10^14 - 10^15 !synapses!

- each neuron received input signals from its !dendrites!

- produces output signals along its single !axon!

- axon branches out and connects via synapses to dendrites of other neurons

- the signals that travel along the axons (x0) interact multiplicatively (e.g. w0x0) 
with dendirites of other neuron based on the synaptic strength at the synapse (w0)

- idea is that synaptic strengths are learnable and control strength of influence

- dendrites carry signal to the cell body where they all get summed

- if final sum is above certain threshold, the neuron can fire, sending a spike along its axon

- we model the firing rate of the neuron with an !activation function f!

- Coarse model - this model of a biological neuron is very coarse: 

a. many different types of neurons, with different properties

b. dendrites in biological neurons perform complex nonlinear computations

c. synapses are not just a single weight, they're complex non-linear dynamical systems

d. exact timing of the output spikes in many systems is known to be important, suggesting
that the rate code approximation may not hold


Single neuron as a linear classifier:

- A single neuron can be used to implement a binary classifier

- e.g. binary Softmax or binary SVM classifiers


Commonly used activation functions/non-linearity:

a. Sigmoid 

- squashes real-valued number into range between 0 and 1

- large neg nums become 0 and large pos number become 1

- has seen frequent use historically since it has a nice interpretation as the firing rate
of a neuron (0 - 1), but has fallen out of favor and is rearely ever used

- two major drawbacks: 


(-) Sigmoid saturate and end up killing gradients

- when neuron's activation saturates at either end of 0 or 1, the gradient at these regions
is almost zero (flat slope)

- recall that during backprop, this local gradient will be multiplied to the gradient of
this gate's output for the whole objective

- therefore if local gradient is very small, it will effectively "kill" the gradient and
almost no signal will flow through the neuron to its weights and recursivle to its data

- one must pay extra caution when initializing the weights of sigmoid neurons to prevent
saturation,

if initial weights are too large then most neurons would become saturated and the network
will barely learned



(-) Outputs are not zero-centered 

- neurons in later layers of processing would be receiving data that is not zero-centered

- this is a problem during gradient descent, because if data coming into a neuron is always
positive, then the gradient on the weights w will either be all positive, or all negative
during backprop

- this could introduce undesirable zig-zagging dynamics in the gradient updates for the 
weights

- however, once these gradients are added up across a batch of data, the final update for the
weights can have variable signs, somewhat mitigating this issue

- thus, this problem has less severe consequences compared to the saturated activation
above


b. Tanh

- squashes a real-valued number to the range [-1, 1] 

- therefore in practice, the tanh non-linearity is always preferred to the sigmoid 
non-linearity

- Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the 
following holds, tanh(x) = 2o(2x) - 1

- pros and cons:

(-) Like the sigmoid neuron, its activations saturate


(+) But unlike the sigmoid neuron, its output is zero-centered


c. ReLU 

- the Rectified Linear Unit has become very popular in the last few years

- computes the function f(x) = max(0,x)

- in other words, the activation is simple thresholded at zero

- pros and cons:

(+) Found to greatly accerlerate the convergence of SGD compared to sigmoid and tanh
    functions. Argued that it is due to its linear, non-saturating form 

(+) Compared to tanh/sigmoid neurons that involve expenseive operations, the ReLU can 
    be implemented by simple thresholding a matrix of activations at zero

(-) Unfortunately, ReLU units can be fragile during training and can "die".

    - for example, a large gradient flowing through a ReLU neuron could cause the weights 
      to update in such a way that the neuron will never activate on any datapoint again

    - if this happens, then the gradient flowing through the unit will forever be zero 
      from that point on. That is, the ReLU units can irreversible due during training 
      since they can get knocked off the data manifold.

    - for example, you may find that as much as 40% of your network can be "dead"
      (i.e. neurons that never activate across the entire training dataset) if the learning
      rate is set too high

    - with a proper setting of the learning rate this is less frequently an issue

d. Leaky ReLU

    - Leaky ReLUs are one attempt to fix the "dying ReLU" problem

    - Instead of the function being zero when x < 0, a leaky ReLU will instead have a small
      negative slope (of 0.01, or so)

    - That is, the function computse f(x) = 1(x < 0)(ax) + 1(x >= 0)(x) where a is a small
      constant

    - some people report success with this form of activation function,but the results are
      not always consistent

    -  the slope in the negative region can also be made into a parameter of each neuron,
       as seen in PReLU neurons

e. Maxout 

	- other types of units have been proposed that do not have the functional form 
	  where a non-linearity is applied on the dot product between the weights and the data

	- one relatively popular choice is the Maxout neuron (introduced recently by 
	  Goodfellow et. al.) that generalizes the ReLU and its leaky version.

	- the Maxout neuron computes the function max(w1x + b1, w2x + b2)

	- notice that both ReLU and Leaky ReLU are a special case of this form, (for example,
      for ReLU we have w1,b1 = 0)

	- the Maxout neuron enjoys all benefits of a ReLU unit (linear regime of operation,
	  no saturation) and does not have its drawbacks (dying ReLU)

	- However, unlike the ReLU neurons it doubles the number of parameters for every single
	  neuron, leading to a high total number of parameters

<TLDR:> - use the ReLU non-linearity
		- be careful with your learning rates if you do so 
		- possibly monitor the fraction of "dead" units in the network 
		- of this concerns oyu, give Leaky ReLU or Maxout a try
		- never use sigmoid, try tanh but expect it to work worse than ReLU/Maxout


Neural Network Architectures:

a. Layer Wise Organization

- most common layer type is the #fully-connected# layer in which neurons between two
  adjacent layers are fully pairwise connected

- #Naming conventions# input layer is not counted

- therefore, a single-layer neural network describes a network with no hidden layers
  (input directly mapped to output)

- in that sense, you can sometimes hear people say the logistic regression or SVMs are 
  simply a special case of single-layer neural networks 

- unlike all layers in a neural network, the output layer neurons most commonnly do not
  have an activation function

- this is because the last layer is usualy reserved to to represent the class scores 
   (e.g. classification)

- the two metrics that people commonly use to measure the size of neural networks are the:

	1. number of neurons
	2. more commonly, the number of params 

	* first network (left) has 4 + 2 = 6 neurons(not counting the inputs), [3x4] + [4x2]
	  = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters

- to give some context, modern Convolutional Networks contain on orders of 100 million 
  parameters and are usuallu made up of approximately 10-20 layers (hence deep learning)

b. Example feed-forward computations

	[py]
		# forward-pass of a 3-layer neural network:
		f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
		x = np.random.randn(3, 1) # random input vector of three numbers (3x1)
		h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)
		h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)
		out = np.dot(W3, h2) + b3 # output neuron (1x1)
	[end]

	- the forward pass of a fully-connected layer corresponds to one matrix multiplication
	  followed by a bias offset and activation function


c. Representational Power

	- one way to look at neural networks with FC layers is that they define a family of
	  functions that are parameterized by the weights of the network 

	- a natural question that arises is: 

	  <What is the representational power of this family of functions?>

	  <In particular, are there functions that cannot be modeled with a Neural Network?>

	- it turns out that neural nets with at least one hidden layer are universal approx.

	- 

d. Setting number of layers and their sizes

	- note that as we increase the size and number of layers in a Neural Network, the
	  # capacity # of the network increases

	- that is, the space of representable functions grows since the neurons can collaborate
	  to express many different functions 

	- neural networks with more neurons can express more complicated functions

	- this is both a blessing and a curse however

	- for example, the model with 20 hidden neurons fits all the training data but at the 
	  cost of segmenting the space into many disjoint red and green decision regions 

	- the model with 3 hidden neurons only has the representational power to classify the
	  data in broad strokes

	- it models the data as two blobs and inteprets the few red points inside the green
	  cluster as # outliers (noise) #

	- in practice, this could lead to better # generalization # on the test set 

	- based on our discussion above, it seems that smaller neural networks can be 
	  preferred if the data is not complex enough to prevent overfitting

	- however, this is incorrect, there are many other preferred ways to prevent overfitting
      in neural networks that we will discuss later (L2 reg, dropout, input noise)

	- in practice, it is always better to use these methods to control overfitting instead
	  of the number of neurons

	- the subtle reason behind this is that smaller networks are harder to train with local
	  methods such as Gradient descent

	- it's clear that their loss functions have relatively few local minima but it turns
	  out that many of these minima are easier to converge to, but they are bad
	  (i.e. high loss)

	- conversely, bigger neural nets contains significantly more local minima, but these
	  minima turn out to be much better in terms of their actual loss

	- since neural nets are non-convex, it's hard to study these properties mathematically

	- but some attempts to understand these objective functions have been made, e.g. in 
	  a recent paper "The Loss Surfaces of Multilayer Networks"

	- in practice, what you find is that if you train a small network the final loss can 
	  display a good amount of variance

	- in some cases you get lucky and converge to a good place but in some cases you get 
	  trapped in one of the bad minima

	- conversely, if you train a large network you'll start to find many different solutions
	  but the variance in the final achieved loss will be much smaller. In other words, all
	  solutions are about equally as good and rely less on the luck of random initialization

	- to reiterate, the regularization strength is the preferred way to control the 
	  overfitting of a neural network

	<TLDR>: The takeaway is that you should not be using smaller networks because you are 
			afraid of overfitting, instead you should use as big of a neural network as your
			computational budget allows, and use other regularization techniques to control
			overfitting			


Summary:

1. introduced a very coarse model of a biological neuron 
2. several types of activation funccs that are used in practice (ReLU) being the most
   common choice
3. introduced neural nets where neurons are connected with Fully-Connected Layers
4. we saw that this layered architecture enables very efficient evaluation of Neural Nets 
   based on matrix multiplications interwoven with the application of the activation
   function 
5. Neural Nets are # universal function approximators # but we also discussed the fact that 
   this property has little to do with their ubiquitous use. They are used because they make
   certain "right" assumptions about the functional forms of functions that come up in 
   practice
6. Larger networks will always work better than smaller networks, but their higher model 
   capacity must be appropriately addressed with stronger regularization(such as higher
   weight decay), or they might overfit.    

   We will see more forms of regularization (especially dropout) in later sections.



-----------------------------------------------------------
  Neural Networks Part 2: Setting up the Data and the Loss
-----------------------------------------------------------

Setting up the data and the model:


a. Data Preprocessing

- 3 common forms of data preprocessing a data matrix X (assume size [N x D]):

  1. Mean subtraction

	- most common form of preprocessing 

	- involves subtracting the mean across every individual feature in the data 

	- has the geometric interpretation of centering the cloud of data around the origin 
	  along every dimension

	[py] 
		X -= np.mean(X, axis = 0) 
	[end]

  2. Normalization 

	- refers to normalizing the data dimensions so that hey are of approximately the same 
  	  scale

	- two common ways of achieving this normalization

		I. Divide each dimension by its standard deviation, once it has been zero-centered:

		   [py] 
		   		X /= np.std(X, axis=0) 
		   [end]

		II. Another form of this preprocesssing normalizes each dimension so that the min
			and max along each dimension is -1 and 1 respectively

	- it only makes sense to apply this preprocessing if you have a reason to 
			  believe that different input features have different scales

	- in the case of images, the relative scales of pixels are already approx. equal
	  (and in range from 0 to 255)


  3. PCA and Whitening 

	- another form of preprocessing 

	- in this process, the data is first centered as described above

	- then the covariance matrix is computed that tells us about the correlation structure
  	  in the data

  	 [py]
  	  # Assume input data matrix X of size [N x D]
	  X -= np.mean(X, axis = 0) # zero-center the data (important)
	  cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix
  	 [end]

  	- the (i,j) element of the data covariance matrix maintains the covariance between 
  	 i-th and j-th dimension of the data 

    - in particular, the diagonal of this matrix contains the variances. 

    - Furthermore, the covariance matrix is symmetric and positive semi-definite

    - etc. etc. didn't complete the notes since PCA/Whitening are not used with ConvNets 

  <Common pitfall>: any preprocessing statistics (e.g. the data mean) must only be computed
  					on the training data, and then applied to the validation / test data 
  					
  					e.g. computing the mean and subtracting it from every image across the 
  					entire datset and then splitting the data into tran/val/test splits 
  					would be a mistake. Instead, the mean must be computed only over the 
  					training data and then subtracted equally from all the splits 
  					(train/val/test)


b. Weight Initialization 

	I. Pitfall: all zero initialization. With proper data normalization, it is reasonable
	   to assume that approximately half of the weights will be psoitivve and half of them
	   will be negative.

		- zero might have the expectation to be the "best guss" but this is a mistake, 
		  because if every neuron in the network computes the same output, then they will 
		  also all compute the same gradients during backprop and udnergo the exact same 
		  param updates

		- in other words, there is no source of asymmetry between neurons if their weights 
		  are initialized to be the same

    II. Small random numbers

		- thus, we still want the weights to be very close to zero, but as we have argued 
		  above, not identically zero.

		- As a solution, it is common to initialize the weights of the neurons to small 
		  numbers and refer to doing so as symmetry breaking

		- the idea is that neurons are all random and unique in the beginning so they will 
		  compute distinct updates and integrate themselves as diverse parts of the full
		  network

		- [py] W = 0.01* np.random.randn(D,H) 
		  [end]

		- where randn samples from a zero mean, unit standard dev. gaussian. With this
		  formulation, every neuron's weight vector is initialized as a random vector 
		  sampled from a multi-dimensional gaussian, so the neurons point in random 
		  directions in the input space

		- it is also possible to use small numbers drawn from a uniform distribution,
		  but this seems to have relatively little impact on the final performance in
		  practice

		- #WARNING#: not necessarily that smaller nums will work strictly better. For 
		  example, a neural netwok that has very small weight will compute very small 
		  gradients on its data. this could greatly diminish the "gradient signal" 
		  flowing backward through a network

	III. Calibrating the variances with 1/sqrt(n)

		- one problem is that the distribution of the outputs from a randomly
		initialized neuron has a variance that grows with number of inputs

		- we can normalize variance of each neuron's output to 1 by scaling its weight
		vector by the square root of its fan-in(number of inputs)

		- ensures all neurons in network initially have approximately the same 
		output distrib and empirically improves rate of convergence

		[py] 

			w = np.random.randn(n) / sqrt(n)

		[end]

	
	IV. Sparse Initialization

		- another way to solve uncalibrate variances problem

		- set all W matrices to zero

		- but to break symmetry every neuron must be randomly connected 
		to a fixed num of neurons below it. 

		- typical num of neurons to connect to may be as small as 10

	V. Initializing the biases

		- possible and common to initialize biases to zero

		- since asymmetry breaking is provided by small random nums in the weights

		- for ReLU non-linearities, some like to use a small value like 0.01 for all
		biases cause this ensures all ReLU units fire at the start and this, 
		obtain and propagate some gradient

		- but no clear eveidence that this provides consistent improvement

		- more common to simple use 0 bias initialization 

    VI. In practice

    	- curr recommendation is use ReLU units and use 
    	  [py]

    	  	w = np.random.randn(n) * sqrt(2.0/n)

    	  [end]

  		- this is as discuessed in He et al.

  	VII. Batch Normalization 

  		- alleviates headeaches with properly initializing neural nets 

  		- explicitly forces activations in the network to take on a unit gaussian 
  		distribustion at the start of training

  		- idea is this is possible cause normalization is a simple differentiable 
  		operation 

  		- usually insert BatchNorm layer immediately after fully connected layers 
  		(or convolutional layers) and before non-linearities

  		- networks that use this are significantly more robust to bad initialization 

  		- can be interpreted as doing preprocessing at every layer of the network
  		but integrated into the network itself in a differentiable manner

c. Regularization 

	I. L2 regularization

		- most common form of reg 

		- can be implemented by penalizing the squared magnitude of all params 

		- that is, for every weight w in the network, we add the 
		(1/2) * lambda * w^2 to the objective where lambda is reg strength

		- common to see 1/2 in front to cancel the 2 that results from the 
		differentiation

		- intuitive interpretation of heavily penalizing peaky weight vectors 

		- preferring diffuse weight vectors 

		- desirable property of encouraging network to use all of its inputs a little
		rather than some of its inputs a lot 

		- notice during grad desc every weight is decayed linearly W += -lambda * W 

	II. L1 regularization 

		- add the term lambda | w | to the objective 

		- possible to combine L1 and L2 regularization called # Elastic net reg #

		- can lead the weight vectors to become sparse during optimization 
		(unlike L2 which discourages sparsity). L1 DOES NOT encourage sparsity, 
		it's more that L2 discourages it

		- if not conerned with explicit feature selection, L2 reg can be expected 
		to give superior performance over L1 

	III. Max norm constraints

		- enforces an absolute upper bound on the magnitude of weight vector for 
		every neuron

		- uses projected gradient descent to enforce the constraint 

		- this corresponds to performing parameter update as normal, then enforcing
		the constraint by clamping the weight vector of every neuron to satisfy 
		||w||2 < c 

		- typical vals of c are on orders of 3 or 4 

		- some people report improvements 

		- appealing property where network cannot explode even when learning rates 
		are set too high because updates are always bounded

	IV. Dropout 

		- no one uses this anymore, skipped 

	V. Bias regularization

		- not common to regularize bias because they do not interact with data through 
		multiplicative interactions

		- does not have the interpretation of controlling the influence of a data dimension
		on the final objective

		- in practical applications, regularizing the bias rarely leads to significantly
		worse performance 

		- this is likely because there are very few bias terms compared to all the weights
		so the classifier can "afford to" use the biases if it needs them to obtain a
		better data loss

	VI. Per-layer regularization

		- not very common to regularize different layers to different amounts

		- relatively few results regarding this idea have been published in the 
		literature 

	<In practice:> most common to use a single, global L2 regularization strength 
	that is cross-validated. The value of p = 0.5 is a reasonable default, but this 
	can be tuned on validation data 


d. Loss Functions 

	- regularization loss has been discussed, second part is the data loss which
	is a supervised learning problem that measures compatibility between a pred 
	and the ground truth label 

	- data loss takes the form of an average over the data losses for every 
	individual example. That is, L = (1/N) summation of (Li)

	- where N is the number of training data 

	I. Classification

		- dataset of examples and a single correct label out of a fixed set for 
		each example 

		- commonly seen cost functions in this setting is the SVM (hinge loss)

		- also, some people report better perform with the squared hinge loss 

		- another common choice is the softmax classifier that uses cross-entropy
		loss

		# Problem: large number of classes #

			- when set of labels is very large (e.g. words in dictionary, ImageNet
			which has 22,000 categories) it may be helpful to use *Hierarchical Softmax*

			- this decomposes labels into a tree 

			- each label is represented as a path along the tree 

			- a Softmax classifier is trained at every node of the tree to disambiguate
			between left and right branches

			- structure of tree strongly impacts performance and is generally problem-
			dependent

		# Attribute classification

			- both losses above assume there is a single correct answer yi

			- what if yi is a binary vector where every example may or may not 
			have a certain attribute, and where the attributes are not exclusive?

			- a sensible approach in this case is to build a binary classifier for 
			every single attribute independently 

			- an alternative to this loss would be to train a logistic regression 
			classifier for every attribute independently 

	II. Regression 

		- task of predicting real-valued quantities such as the price of houses

		- common to compute the loss between predicted quantity and true answer then,
		measure the L2 squared norm, or L1 norm of the difference

		# Word of caution: # 

			- important to note that the L2 loss is much harder to optimize than
			a more stable loss such as Softmax

			- requires a very fragile and specific property from the network to  
			output exactly one correct value for each input

			- not the case with Softmax, where the precise value of each score is less
			important

			- only matters that their magnitudes are appropriate

			- L2 loss is less robust because outliers can introduce huge gradients

			- when faced with a regression problem, *consider if absolutely necessary*

			- instead, have a strong preference to discretizing outputs to bins and 
			perform classification over them whenver possible

			e.g. predicting star rating, it might be better to use 5 independent 
			classifiers for ratings of 1-5 stars instead of regression 

			- classification has the additional benefit that it can give you a distrib 
			over the regression outputs, not just a single output with no indication 
			of its confidence

			- if certain that classification is not appropriate, use L2 but be careful.

			- for example, L2 is more fragile and applying dropout in the network 
			is not a great idea



---------------------------------------------------
  Neural Networks Part 3: Learning and Evaluation
---------------------------------------------------


Babysitting the learning process:

  IV. Activation/Gradient distributions per layer

    - wrong initialization can slow down or stop the learning process
    
    - issue can be diagnosed easily

    - one way is plot activation/gradient histograms for all layers of the
      network

    - not a good sign to see any strange distributions

	e.g. w/ tanh neurons, 

	* expected to see distribution of neuron activations
	between full range of [-1,1]

	* instead of seeing all neurons outputting zero, or all neurons being 
	completely saturated at either -1 or 1

  V. First-layer Visualizations

    - when working with image pixels

    - helpful and satisfying to plot first-layer features visually

    - noisy features could be a symptom: 

	* unconverged network 

	* improperly set learning rate

	* very low weight regularization penalty

    - nice, smooth, clean and diverse features are good indications that training
      is doing well


Parameter Updates:

  I. SGD and bells and whistles

    a. Vanilla update 

       - simplest form of update which changes parameters along 
      	 negative gradient direction

       - x += - learning_rate * dx

       

    b. Momentum update 

	     - motivated from a physical perspective of the optimization problem

	     - loss can be interpreted as height of hilly terrain

       - initializing params with rand numbers is like setting a particle w/ 0 initial 
       velocity at some location

       - optimization process can then be seen as equivalent to process of simulating the 
       parameter vector as rolling on the landscape


  II. Annealing the learning rate

    - w/ high learning rate the system contains too much kinetic energy & the parameter 
    vector bounces around chaotically, unable to settle down into deeper but narrower
    parts of the loss function

    - knowing when to decay learning rate can be tricky

    - decay it slowly, will waste computation bouncing around chaotically w/ little
    improvement for a long time

    - decay it aggressively, system will cool too quickly,unable to reach best position it
    can 

    - 3 common types of implementing learning rate decay:

      a. Step decay

        - reduce lr by some factor every few epochs

        - typical values: reducing lr by a half every 5 epochs or by 0.1 every 20 epochs

        - these numbers depend heavily on type of problem and the model

        - <heuristic>: watch validation error while training w/ fixed lr, reduce lr by 
        a constant (0.5) whenever validation error stops improving

      b. Exponential decay 

        - has mathematical form a = a0 * e^-kt, where a0,k are hyperparams and t is iteration
        number


      c. 1/t decay

        - has mathematical form a = a0 / (1 + kt) where a0,k are hyperparams and t 
        is iteration number 

    - <heuristic>: in practice, step decay is slightly preferable because the hyperparams
    it involves are more interpretable than hyperparameter k 

    - <heuristic>: if can afford computational budget, err on the side of slower decay
    and train for longer time


  III. Second order methods

    - based on Newton's method 

    - iterates the following update: x <- x - [Hf(x)]^-1 ∇(f(x))

    - Hf(x) is the hessian matrix: square matrix of 2nd-order partial derivatives of the 
    function 

    - term ∇(f(x)) is the gradient vector, as seen in gradient descent

    - Hessian describes local curvature of loss function, which allows a more efficient 
    update 

    - multiplying by inverse Hessian leads optimization to take aggressive steps in 
    directions of shallow curvature and shorter steps in directions of steep curvature

    - note absence of any lr hyperparameters in the update formula, which proponents of 
    these methods cite as a large advantage over first-order methods

    - however, the update above is impractical for most deep learning applications because
    computing and inverting Hessian in its explicit form is very costly in both space & time

    - for instance, a NN w/ one million params would have a Hessian matrix of size 
    [1,000,000 x 1,000,000], occupying approx. 3725 GB of RAM

    - thus, large variety of quasi-Newton methods have been developed that seek to approx
    inverse Hessian 

    - most popular is L-BFGS which uses the information in gradients over time to form 
    approximation implicitly  (i.e. full matrix is never computed)

    - however, even after eliminate memory concerns, a large downside of a naive application
     of L-BFGS is that it must be computed over entire training set which could contain 
     millions of examples

    - unlike mini-batch SGD, getting L-BFGS to work on mini-batches is more tricky and an 
    active area of research

    - <In practice>: not common to see L-BFGS or similar second-order methods applied to 
    large-scale Deep Learning and ConvNets. Instead, SGD variants based on Nesterov's 
    momentum are more standard because they are simpler and scale more easily.


  IV. Per-parameter adaptive lr methods

    - all previous approaches manipulated lr globally and equally for all params 

    - tuning lr is an expensive process, so much work has gone into devising methods that
    can adaptively tune the lr and even do so per parameter


    I. Adagrad
  
      - an adaptive lr method originally proposed by Duchi et. al.

      [py]
        #Assume the gradient dx and parameter vector x
        cache += dx ** 2
        x += - learning_rate * dx / (np.sqrt(cache) + eps)

      [end] 

      - "cache" has size equal to the size of the gradient, keeps track of per-parameter 
      sum of squared gradients

      - then used to normalize the parameter update step, element-wise

      - weights that receive high gradients will have their effective lr reduced

      - weights that receive small or infrequent updates will have their effective lr 
      increased

      - amusingly, the sqr root operation turns out to be very important & without it 
      the algorithm performs much worse 

      - smoothing term "eps" avoids division by zero 

      - eps usually set somewhere from 1e-4 to 1e-8

      - downside of Adagrad is that in Deep Learning, monotonic learning rate usually 
      proves too aggressive and stops learning too early 


    II. RMSprop 

      - very effective but currently unpublished adaptive lr method

      - everyone who uses this citse slide 29 of lecture 6 of Geoff Hinton's Coursera 
      class

      - In particular, uses a moving average of squared gradients instead, giving: 

      [py]

        cache = decay_rate * cache + (1 - decay_rate) * dx**2
        x += - learning_rate * dx / (np.sqrt(cache) + eps)

      [end]

    - "decay_rate" is a hyperparam and typical values are [0.9,0.99,0.999]

    - notice that "X+=" update is identical to Adagrad but the "cache" variable is a leaky

    - hence, RMSProp still modulates the lr of each weight based on the magnitudes of its 
    gradients, which has a beneficial equalizing effect 

    - unlike Adagrad, the updates do not get monotonically smaller 


    III. Adam

      - recently proposed update 

      - looks a bit lke RMSProp with momentum 

      - the (simplified) update looks as follows:

      [py]

        m = beta1*m + (1-beta1) * dx
        v = beta2*v + (1-beta2) * (dx**2)
        X += - learning_rate * m / (np.sqrt(v) + eps)

      [end]

      - notice that update looks exactly as RMSProp update, except the "smooth" version
      of the gradient "m" is used instead of the raw gradient vector dx

      - recommended values in the paper are eps = 1e-8, beta1 = 0.9, beta2 = 0.999

      - <In practice>: Adam is currently recommended as default algorithm to use and often
      works slightly better than RMSprop. However, worth trying SGD + Nesterov Momentum as 
      an alternative

      - full Adam update also includes a bias correction mechanism, which compensates for
      the fact that in the first few time steps the vectors m,v are both initialized and 
      therefore biased at zero, before they fully "warm up"

      - with bias correction mechanism, update looks as follows:

      [py]
        # t is your iteration counter going from 1 to infinity
        m = beta1*m + (1-beta1)*dx
        mt = m / (1-beta1**t)
        v = beta2*v + (1-beta2)*(dx**2)
        vt = v / (1-beta2**t)
        x += - learning_rate * mt / (np.sqrt(vt) + eps)
      [end]

      - update is now a function of the iteration as well as other params 


Hyperparameter optimization:

  - training Neural Nets can involve many hyperparam settings 

  - most common hyperparams in context of Neural Nets include:

    a. initial lr
    b. lr decay schedule (such as the decay constant) 
    c. reg strength (L2 penalty, dropout strength)

  I. Implementation 

    - larger neural nets typically require a long time to train 

    - performing hyperparam search can take many days/weeks

    - important to keep in mind since it influences the design of your code base 

    - a particular design is to have a # worker # that continuously samples random 
    hyperparams and performs optimization

    - during training, worker will keep track of validation performance after every epoch 
    and writes a model checkpoint to a file (preferably on a shared file system)

    - useful to include validation performance directly in the file name 

    - then a second program called # master # which launches or kills workers across a 
    computing cluster, and may inspect checkpoints written by workers and plot their  
    training statistics, etc.


  II. Prefer one validation fold to cross-validation

    - most cases, a single validation set
    of respectable size substantially simplifies the code base without need for 
    cross-validation

    - people will say they "cross-validated" a param, but many times it's assumed that
    they only used a single validation set


  III. Hyperparameter ranges

    - search for hyperparams on log scale 

    - for example, a typical sampling of lr would look as follows:

      [py]
        learning_rate = 10 ** uniform(-6, 1)
      [end]

    - that is, we are generating a random num from a uniform distrib, but then raising
    it to the power of 10 

    - same strategy should be used for regularization strength

    - intuitively, because lr and reg strength have multiplicative effects on training 
    dynamics 

    - for example, fixed change of adding 0.01 to a lr has huge effects on dynamics if 
    lr is 0.001, but nearly no effect if lr is 10

    - this is cause the lr multiplies the computed grad in the update 

    - therefore much more natural to consider range of lr multiplied or divided by some 
    value, than a range of lr added or subtracted to by some value 


  IV. Prefer random search to grid search 

    - as argued by Bergstra and Benigio,

    - "randomly chosen trials are more efficient for hyper-parameter optimization than
    trials on a grid" 

    - as it turnsout, this also usually easier to implement 


  V. Careful with best values on border

    - sometimes it can happen you're searching for a hyperparam in a bad range 

    - for instance, suppose 

      [py]
        learning_rate = 10 ** uniform (-6, 1)
      [end]

    - once we receive results, important to double check that final lr is not at edge of
    this interval, otherwise may be missing more optimal hperparam setting beyond 
    the interval 


  VI. Stage your search from coarse to fine 

    - can be helpful to first search in coarse ranges

    - e.g. 10 ** [-6,1]

    - then depending on where best results are turning up, narrow the range 

    - can be helpful to perform initial coarse search while only training for 1 epoch 
    or even less, cause many hyperparam settings can lead the model to not learn at all 
    or immediately explode with infinite cost 

    - second stage could then perform a narrower search with 5 epochs 

    - last stage could perform a detailed search in final range for many more epochs 

  VII. Bayesian Hyperparameter Optimization 

    - whole area of research devoted to coming up with algorithms that try to more 
    efficiently navigate space of hyperparams 

    - core idea is to appropriately balance exploration (exploitation trade-off when 
      querying the performance at different hyperparams)

    - many librarise have been developed based on this: Spearmintm, SMAC, Hyperopt

    - <In practice>: w/ ConvNets it is still relatively difficult to beat rand 
    search in a carefully-chosen interval  


Evaluation:

  I. Model Ensembles 

    - reliable approach to improving neural net performance by a few percent is to train 
    multiple independent models 

    - then average predictions at test time 

    - as number of models in the ensemble increases, performance typically monotonically
    improves (though with diminishing returns)

    - improvements are more dramatic w/ higher model variety in the ensemble 

    - few approaches to forming an ensemble:

      a. Same model, different initializations

        - use cross-validation to determine best hyperparams

        - train multiple models w/ best set of hyperparams but w/ different random 
        initialization

        - danger with this approach is the variety is only due to initialization 

      b. Top models discovered during cross-validation

        - use cross-validation to determine best hyperparams

        -pick the top few models to form the ensemble 

        - improves variety of ensemble but has the danger of including suboptimal models 

        - <in practice>: can be easier to perform since doesn't require additional 
        retraining of models after cross-validation

      c. Different checkpoints of a single model 

        - if training is very expensive, people have had limited succes in taking different
        checkpoints of a single network over time (for example after every epoch) & using
        those to form an ensemble 

        - clearly suffers from lack of variety, but can still work reasonably well in 
        practice

        - advantage of this approach is that it is very cheap 

      d. Running average of params during training 

        - cheap way of almost always getting extra percent or two of performance
        
        - maintain second copy of network's weights in memory that maintains an exponentially
        decaying sum of previous weights during training 

        - this way you're averaging state of the network over last several iterations 

        - will find that this "smoothed" version of the weights over last few steps almost
        always achieves better validation error

        - rough intuition is that the objective is bowl-shaped and your network is jumping
        around the mode, so average has a higher chance of being somewhere nearer the mode

    - one disadvantage of model ensembles is that they take longer to evaluate on test 
    example

    - may find recent work from Geoff Hinton on "Dark Knowledge" inspiring, where idea is to 
    "distill" a good ensemble back to a single model by incorporating the ensemble log 
    likelihoods into a modified objective
